{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed50d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0de13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4624b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOllama(base_url='http://localhost:11434',model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d0a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import redis\n",
    "\n",
    "r = redis.Redis(\n",
    "    host='redis-13432.c52.us-east-1-4.ec2.redns.redis-cloud.com',\n",
    "    port=13432,\n",
    "    decode_responses=True,\n",
    "    username=\"default\",\n",
    "    password=\"\",\n",
    ")\n",
    "\n",
    "success = r.set('foo', 'bar')\n",
    "# True\n",
    "\n",
    "result = r.get('foo')\n",
    "print(result)\n",
    "# >>> bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache MESSAGE >40\n",
    "summarizer_prompt\n",
    "insert_into_vertex_vector_db(\n",
    "    \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4ef3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_model = OllamaEmbeddings(model='snowflake-arctic-embed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1a5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGE_LIMIT=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ebcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_message(user, message):\n",
    "    \"\"\"Add message to Redis (cache).\"\"\"\n",
    "    r.rpush(\"chat_history\", f\"{user}: {message}\")\n",
    "    # if above threshold → summarize & store into FAISS\n",
    "    if r.llen(\"chat_history\") > MESSAGE_LIMIT:\n",
    "        flush_to_vertex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7f367f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import redis_prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate,PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b9dc99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.vector_db import insert_into_vertex_vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_to_vertex():\n",
    "    \"\"\"Take messages from Redis, join them, push to FAISS, clear cache.\"\"\"\n",
    "    history = r.lrange(\"chat_history\", 0, -1)\n",
    "    if not history:\n",
    "        return\n",
    "    \n",
    "    ### yha pe llm hoga \n",
    "    summarizer_prompt = PromptTemplate.from_template(redis_prompt) \n",
    "    chain=summarizer_prompt | llm\n",
    "\n",
    "    response=chain.invoke({'conversation_transcript':history})    \n",
    "\n",
    "userid\n",
    "\n",
    "conversation\n",
    "\n",
    "    ### gotta add like a fucking list to it and metadata too\n",
    "    insert_into_vertex_vector_db(\n",
    "        text_chunks=[response.content],\n",
    "        metadata_list=\"\",\n",
    "        index_id=\"3761581011226329088\"  # Your index ID\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Simple summary = just join for demo (replace with LLM summary if needed)\n",
    "    # faiss.add_texts(history)\n",
    "\n",
    "\n",
    "    # # Add to FAISS\n",
    "\n",
    "\n",
    "    # # Save FAISS\n",
    "    # faiss.save_local(vectorstore_path)\n",
    "\n",
    "    # Clear Redis\n",
    "    r.delete(\"chat_history\")\n",
    "\n",
    "    print(\"✅ Flushed Redis -> FAISS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bae4b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_memory(query, k=2):\n",
    "    \"\"\"Search long-term FAISS memory.\"\"\"\n",
    "    \n",
    "    results = faiss.similarity_search_with_score(query, k=k)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3f53d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"user\", \"hey, how are you?\")\n",
    "# add_message(\"agent\", \"I’m fine, monkey.\")\n",
    "# add_message(\"user\", \"tell me about faiss?\")\n",
    "# add_message(\"agent\", \"faiss is for similarity search.\")\n",
    "# add_message(\"user\", \"and redis?\")\n",
    "# add_message(\"agent\", \"redis is an in-memory cache.\")\n",
    "\n",
    "# # After this, messages will flush into FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f4c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_docs = list(faiss.docstore._dict.values())\n",
    "\n",
    "# for i, doc in enumerate(all_docs, 1):\n",
    "#     print(f\"\\n--- Document {i} ---\")\n",
    "#     print(\"Content:\", doc.page_content)\n",
    "#     print(\"Metadata:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6101b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Messages in Redis:\n",
      "agent: yes. it's a piece of disinformation. the goal isn't to inform you, it's to make you scared and angry to get more clicks and shares.\n",
      "user: damn, that's messed up. so it's 100% fake.\n",
      "agent: correct. no credible sources have ever reported anything like this. it's a fabricated story.\n",
      "user: alright, good to know. thanks for breaking that shit down for me.\n",
      "agent: no problem. always question the source and look for proof.\n",
      "\n",
      "Parsed Messages: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = r.lrange(\"chat_history\", 0, -1)\n",
    "print(\"\\nMessages in Redis:\")\n",
    "for m in messages:\n",
    "    print(m)\n",
    "\n",
    "# If you stored JSON objects:\n",
    "parsed = [json.loads(m) for m in messages if m.startswith(\"{\")]\n",
    "print(\"\\nParsed Messages:\", parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97ec48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response is : content=\"**Summary**: The user seeks advice on selecting a good embedding model and is provided with two options by the agent.\\n\\n**User Goal**: To determine which embedding model to use.\\n\\n**Final Outcome**: Information Provided - The user was given guidance on selecting an embedding model, specifically suggestions for OpenAI's text-embedding-3-small or instructor models.\" additional_kwargs={} response_metadata={'model': 'llama3', 'created_at': '2025-09-04T16:11:15.1906509Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14368680100, 'load_duration': 7990149500, 'prompt_eval_count': 255, 'prompt_eval_duration': 1068787200, 'eval_count': 70, 'eval_duration': 5299953100, 'model_name': 'llama3'} id='run--073f5ece-f91e-4a91-aeb2-7f5847964449-0' usage_metadata={'input_tokens': 255, 'output_tokens': 70, 'total_tokens': 325}\n",
      "✅ Flushed Redis -> FAISS\n",
      "response is : content='**Summary**: The conversation analyzes the credibility of an article claiming microchips are being added to cereal, concluding that the source and tactics used are suspicious and likely indicative of misinformation.\\n\\n**User Goal**: To determine if the headline \"They\\'re Now Putting Microchips in Our Cereal!\" is real or fake.\\n\\n**Final Outcome**: Information Provided - The agent debunks the article as likely false, highlighting flaws in the source and writing style.' additional_kwargs={} response_metadata={'model': 'llama3', 'created_at': '2025-09-04T16:11:27.7610057Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7857272600, 'load_duration': 204344900, 'prompt_eval_count': 290, 'prompt_eval_duration': 944759800, 'eval_count': 90, 'eval_duration': 6707661200, 'model_name': 'llama3'} id='run--5425c8c3-ac5f-4813-8b1f-7e6e8fb95157-0' usage_metadata={'input_tokens': 290, 'output_tokens': 90, 'total_tokens': 380}\n",
      "✅ Flushed Redis -> FAISS\n"
     ]
    }
   ],
   "source": [
    "add_message(\"user\", \"bro, my uncle sent me this article from 'FreedomEagle.net'.\")\n",
    "add_message(\"user\", \"headline is 'They're Now Putting Microchips in Our Cereal!' -- is this real or fake?\")\n",
    "add_message(\"agent\", \"okay, let's analyze that. first red flag is the source, 'FreedomEagle.net'. it's not a real news organization, it's a known conspiracy blog.\")\n",
    "add_message(\"user\", \"but it looks professional, they have pictures and stuff.\")\n",
    "add_message(\"agent\", \"that's part of the trick. the article uses vague sources like 'one insider revealed' without any actual names or proof.\")\n",
    "add_message(\"agent\", \"it's also using emotionally loaded words like 'they're' to create an 'us vs. them' feeling. it’s a classic fear-mongering tactic.\")\n",
    "add_message(\"user\", \"so you're saying it's bullshit?\")\n",
    "add_message(\"agent\", \"yes. it's a piece of disinformation. the goal isn't to inform you, it's to make you scared and angry to get more clicks and shares.\")\n",
    "add_message(\"user\", \"damn, that's messed up. so it's 100% fake.\")\n",
    "add_message(\"agent\", \"correct. no credible sources have ever reported anything like this. it's a fabricated story.\")\n",
    "add_message(\"user\", \"alright, good to know. thanks for breaking that shit down for me.\")\n",
    "add_message(\"agent\", \"no problem. always question the source and look for proof.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ee741",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # if state[\"save_to_vector_db\"]:\n",
    "            #     metadata = {\n",
    "            #         \"user_id\": state[\"id\"],\n",
    "            #         \"claim\": claim,\n",
    "            #         \"source_url\": result[\"url\"],\n",
    "            #         \"title\": result[\"title\"]\n",
    "            #     }\n",
    "            #     chunks = chunk_text_by_paragraph(result[\"content\"], max_chunk_size=1000, overlap=200)\n",
    "            #     for chunk in chunks:\n",
    "            #         insert_into_vertex_vector_db(chunk, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375930d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # A completely new dataset about Indian cuisine to avoid matching old data\n",
    "    # sample_texts = [\n",
    "    #     \"Samosas are a popular Indian snack, typically a fried or baked pastry with a savory filling, such as spiced potatoes, onions, and peas.\",\n",
    "    #     \"Butter chicken, or murgh makhani, is a classic Indian curry made with tender chicken in a mildly spiced tomato sauce.\",\n",
    "    #     \"Biryani is a mixed rice dish with its origins among the Muslims of the Indian subcontinent, often reserved for special occasions.\",\n",
    "    #     \"Vada pav is a vegetarian fast food dish native to the state of Maharashtra, consisting of a deep fried potato dumpling placed inside a bread bun.\"\n",
    "    # ]\n",
    "\n",
    "    # sample_metadata = [\n",
    "    #     {\"dish_type\": \"snack\", \"region\": \"North India\", \"spice_level\": 3, \"tags\": [\"vegetarian\", \"fried\"]},\n",
    "    #     {\"dish_type\": \"main_course\", \"region\": \"North India\", \"spice_level\": 2, \"tags\": [\"non-vegetarian\", \"curry\"]},\n",
    "    #     {\"dish_type\": \"main_course\", \"region\": \"Various\", \"spice_level\": 4, \"tags\": [\"rice\", \"celebration\"]},\n",
    "    #     {\"dish_type\": \"snack\", \"region\": \"West India\", \"spice_level\": 5, \"tags\": [\"vegetarian\", \"street_food\"]}\n",
    "    # ]\n",
    "\n",
    "    # insert_into_vertex_vector_db(\n",
    "    #     text_chunks=sample_texts,\n",
    "    #     metadata_list=sample_metadata,\n",
    "    #     index_id=\"3761581011226329088\"  # Your index ID\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "418c6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import redis_prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate,PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f12aaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.vector_db import insert_into_vertex_vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c75df5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.111.0-py2.py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (5.29.5)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (24.2)\n",
      "Collecting google-cloud-storage<3.0.0,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_bigquery-3.36.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_resource_manager-1.14.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting shapely<3.0.0 (from google-cloud-aiplatform)\n",
      "  Using cached shapely-2.1.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (1.26.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (2.11.7)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (4.13.2)\n",
      "Requirement already satisfied: docstring_parser<1 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform)\n",
      "  Using cached google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform)\n",
      "  Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.14.0 (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform)\n",
      "  Using cached grpc_google_iam_v1-0.14.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached google_crc32c-1.7.1-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (4.9.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (8.2.3)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from shapely<3.0.0->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rawat\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.26.20)\n",
      "Using cached google_cloud_aiplatform-1.111.0-py2.py3-none-any.whl (8.0 MB)\n",
      "Using cached google_cloud_bigquery-3.36.0-py3-none-any.whl (258 kB)\n",
      "Using cached google_cloud_resource_manager-1.14.2-py3-none-any.whl (394 kB)\n",
      "Using cached google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "Using cached shapely-2.1.1-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_crc32c-1.7.1-cp312-cp312-win_amd64.whl (33 kB)\n",
      "Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Using cached grpc_google_iam_v1-0.14.2-py3-none-any.whl (19 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Installing collected packages: websockets, shapely, google-crc32c, google-resumable-media, grpc-google-iam-v1, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 10.4\n",
      "    Uninstalling websockets-10.4:\n",
      "      Successfully uninstalled websockets-10.4\n",
      "Successfully installed google-cloud-aiplatform-1.111.0 google-cloud-bigquery-3.36.0 google-cloud-core-2.4.3 google-cloud-resource-manager-1.14.2 google-cloud-storage-2.19.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 grpc-google-iam-v1-0.14.2 shapely-2.1.1 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyppeteer 2.0.0 requires websockets<11.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-aiplatform\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6de950",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_prompt = PromptTemplate.from_template(redis_prompt) \n",
    "chain=summarizer_prompt | llm\n",
    "\n",
    "response=chain.invoke({'conversation_transcript':messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dea49833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['conversation_transcript'], input_types={}, partial_variables={}, template='\\n        You are a conversation summarization service. Your task is to analyze the following transcript between a \"User\" and an \"Agent\" and generate a short and consice summary that captures the key information.\\n        make sure to follow these like these defination for ur process\\n        - `summary`: A concise, one-sentence summary of the entire exchange.\\n        - `user_goal`: What the user was trying to accomplish.\\n        - `final_outcome`: The result of the conversation (e.g., \"Issue Resolved\", \"Information Provided\", \"Question Answered\").\\n\\n\\n        Transcript:\\n        \"\"\"\\n        {conversation_transcript}\\n        \"\"\"\\n        \\n        ')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d3062e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=summarizer_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1d93561",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({'conversation_transcript':messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417243b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON from LLM:\n",
      "content='Here is the output:\\n\\n{\\n    \"summary_of_analysis\": \"A conversation about technical topics related to natural language processing\",\\n    \"verdict\": \"Benign\",\\n    \"confidence_score\": 0.9,\\n    \"malicious_indicators\": [],\\n    \"key_claims_analyzed\": [\"using a good embedding model\", \"embedding models in OpenAI\"],\\n    \"key_entities\": {\\n        \"OpenAI\": {}\\n    }\\n}' additional_kwargs={} response_metadata={'model': 'llama3', 'created_at': '2025-09-04T15:36:06.9596754Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21878087200, 'load_duration': 11939485600, 'prompt_eval_count': 382, 'prompt_eval_duration': 1147831800, 'eval_count': 89, 'eval_duration': 8789654200, 'model_name': 'llama3'} id='run--996e81d8-8332-418c-ae17-ec37d140006a-0' usage_metadata={'input_tokens': 382, 'output_tokens': 89, 'total_tokens': 471}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Parse directly\n",
    "try:\n",
    "    response_json = json.loads(response.content)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Invalid JSON from LLM:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5427eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is the summary:\\n\\n**Summary**: The user sought advice on selecting an embedding model and was recommended OpenAI's text-embedding-3-small or instructor models.\\n\\n**User Goal**: To find a good embedding model.\\n\\n**Final Outcome**: Information Provided - recommendations were given to help the user make an informed decision.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68f63ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: LLM did not return valid JSON. Response was:\n",
      "Here is the summary:\n",
      "\n",
      "**Summary**: The user sought advice on selecting an embedding model and was recommended OpenAI's text-embedding-3-small or instructor models.\n",
      "\n",
      "**User Goal**: To find a good embedding model.\n",
      "\n",
      "**Final Outcome**: Information Provided - recommendations were given to help the user make an informed decision.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    summary_data = json.loads(response.content)\n",
    "    print(\"Successfully parsed LLM summary.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: LLM did not return valid JSON. Response was:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "716e0abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is the summary:\\n\\n**Summary**: The user sought advice on selecting an embedding model and was recommended OpenAI's text-embedding-3-small or instructor models.\\n\\n**User Goal**: To find a good embedding model.\\n\\n**Final Outcome**: Information Provided - recommendations were given to help the user make an informed decision.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc236e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
