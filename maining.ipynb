{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4512ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Verifier_Agent import verifier_tool\n",
    "from langgraph.graph import StateGraph, END,START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, ToolMessage\n",
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory, ChatVertexAI\n",
    "from prompts import main_prompt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from typing import TypedDict, List\n",
    "from Retriever.Retriever_Agent import retriever_agent\n",
    "from Tools.Human_Response import human_response\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# --- Model Configuration (Unchanged) ---\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.28,\n",
    "    \"max_output_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": None,\n",
    "    \"safety_settings\": safety_settings,\n",
    "}\n",
    "model = ChatVertexAI(\n",
    "    model_name=\"gemini-2.5-flash-lite\",\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "# --- Agent State Definition (Unchanged) ---\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    verified_results: str\n",
    "    relevant_context: str\n",
    "    condensed_query: str\n",
    "    image_path: List[str]\n",
    "\n",
    "\n",
    "def router_entry_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"A simple node that acts as the starting point for the routing logic.\"\"\"\n",
    "    print(\"---ENTERING GRAPH, PREPARING TO ROUTE---\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def condense_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Condenses the chat history and latest user query into a standalone question.\"\"\"\n",
    "    print(\"---NODE: CONDENSING QUERY---\")\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    history = state[\"messages\"][:-1]\n",
    "    if not history:\n",
    "        state[\"condensed_query\"] = user_message\n",
    "        print(f\"---CONDENSED QUERY (No history): {user_message}---\")\n",
    "        return state\n",
    "\n",
    "    formatted_history = \"\\n\".join(\n",
    "\n",
    "        [f\"{'Human' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\" for msg in history])\n",
    "    condensing_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"Given the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question, in its original language. Do not answer the question, just reformulate it.\"),\n",
    "        (\"human\", \"Chat History:\\n{chat_history}\\n\\nFollow Up Input: {question}\"),\n",
    "    ])\n",
    "    condenser_chain = condensing_prompt | model\n",
    "    response = condenser_chain.invoke({\"chat_history\": formatted_history, \"question\": user_message})\n",
    "    state[\"condensed_query\"] = response.content\n",
    "    print(f\"---CONDENSED QUERY: {response.content}---\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20448182",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# --- 4. Graph Definition (Final Version) ---\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"router\", router_entry_node)\n",
    "graph.add_node(\"condense_query\", condense_query)\n",
    "\n",
    "graph.add_edge(START,'router')\n",
    "graph.add_edge('router','condense_query')\n",
    "graph.add_edge('condense_query',END)\n",
    "\n",
    "app = graph.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c09a52b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image,display\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m display(Image(\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rawat\\anaconda3\\envs\\final_agent\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:695\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    689\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    690\u001b[39m     curve_style=curve_style,\n\u001b[32m    691\u001b[39m     node_colors=node_colors,\n\u001b[32m    692\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    693\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    694\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rawat\\anaconda3\\envs\\final_agent\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:294\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    288\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    289\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    290\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    291\u001b[39m         )\n\u001b[32m    292\u001b[39m     )\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    302\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rawat\\anaconda3\\envs\\final_agent\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:451\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    447\u001b[39m     msg = (\n\u001b[32m    448\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    455\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image,display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import TypedDict, List, Annotated, Sequence # FIX: Import Annotated and Sequence\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_google_vertexai import (ChatVertexAI, HarmBlockThreshold,\n",
    "                                       HarmCategory)\n",
    "# FIX: Import a working checkpointer and the 'add_messages' helper\n",
    "from Retriever.Retriever_Agent import retriever_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# --- MOCK RETRIEVER FOR TESTING ---\n",
    "# This is a fake retriever so you can test the graph's logic without errors.\n",
    "# Replace this with your actual 'from Retriever.Retriever_Agent import retriever_agent' when ready.\n",
    "\n",
    "# --- Model and Memory Configuration ---\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}\n",
    "\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.28,\n",
    "    \"max_output_tokens\": 1000,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": None,\n",
    "    \"safety_settings\": safety_settings,\n",
    "}\n",
    "model = ChatVertexAI(\n",
    "    model_name=\"gemini-2.5-flash-lite\",\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# FIX 1 (cont.): Use the working SqliteSaver instead of the non-functional base class.\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# --- Agent State ---\n",
    "class AgentState(TypedDict):\n",
    "    # FIX 2: The 'messages' field MUST be wrapped in Annotated for memory to work.\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    relevant_context: str\n",
    "    condensed_query: str\n",
    "    image_path: List[str] # Keeping for future use\n",
    "\n",
    "# --- Node Definitions (Your code is unchanged here) ---\n",
    "\n",
    "def condense_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Condenses chat history into a standalone question.\"\"\"\n",
    "    print(\"---NODE: CONDENSING QUERY---\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"DEBUG: Agent received {len(state['messages'])} message(s) in its state.\")\n",
    "    for i, msg in enumerate(state['messages']):\n",
    "        print(f\"  -> Message [{i}]: Type={msg.type}, Content='{msg.content}'\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    user_message = state[\"messages\"][-1].content\n",
    "    history = state[\"messages\"][:-1]\n",
    "    if not history:\n",
    "        print(\"VERDICT: No history found. Treating as the first message.\")\n",
    "        state[\"condensed_query\"] = user_message\n",
    "        return state\n",
    "    print(\"VERDICT: History found. Rephrasing query based on context.\")\n",
    "    formatted_history = \"\\n\".join(\n",
    "        [f\"{'Human' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\" for msg in history]\n",
    "    )\n",
    "    condensing_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given the conversation and a follow-up question, rephrase the follow-up into a standalone question.\"),\n",
    "        (\"human\", \"Chat History:\\n{chat_history}\\n\\nFollow Up Input: {question}\"),\n",
    "    ])\n",
    "    response = (condensing_prompt | model).invoke({\"chat_history\": formatted_history, \"question\": user_message})\n",
    "    state[\"condensed_query\"] = response.content\n",
    "    print(f\"---CONDENSED QUERY: {response.content}---\")\n",
    "    return state\n",
    "\n",
    "def decide_route(state: AgentState) -> str:\n",
    "    \"\"\"Routes between using the retriever or handling a general conversation.\"\"\"\n",
    "    print(\"---ROUTER: DECIDING ROUTE---\")\n",
    "    query = state[\"condensed_query\"]\n",
    "    \n",
    "    # This improved prompt gives clearer instructions to the router LLM.\n",
    "    router_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert at routing user requests. Classify the user's request into one of two categories:\n",
    "\n",
    "1.  **use_retriever**: The user is asking a clear factual question about a topic, person, place, or event that likely requires searching a knowledge base.\n",
    "    Examples: \"Who is the president?\", \"What is LangGraph?\", \"what is my first prompt\"\n",
    "\n",
    "2.  **general_conversation**: The user is having a regular conversation. This includes greetings, statements, or giving information.\n",
    "    Examples: \"hi\", \"thanks that was helpful\", \"my name is Puneet\"\n",
    "\"\"\"),\n",
    "        (\"human\", \"User request: {user_query}\"),\n",
    "    ])\n",
    "    \n",
    "    route = (router_prompt | model).invoke({\"user_query\": query})\n",
    "    \n",
    "    if \"use_retriever\" in route.content.strip().lower():\n",
    "        print(\"---ROUTE: To RETRIEVE_CONTEXT---\")\n",
    "        return \"use_retriever\"\n",
    "    else:\n",
    "        print(\"---ROUTE: To GENERAL_CONVERSATION---\")\n",
    "        return \"general_conversation\"\n",
    "\n",
    "def retrieve_context(state: AgentState) -> AgentState:\n",
    "    \"\"\"Retrieves context from the vector database.\"\"\"\n",
    "    print(\"---NODE: RETRIEVE CONTEXT---\")\n",
    "    query = state[\"condensed_query\"]\n",
    "    context_result = retriever_agent(query)\n",
    "    chunks = context_result.get(\"retrieved_chunks\", [])\n",
    "    state[\"relevant_context\"] = \"\\n\\n\".join([chunk.get(\"text\", \"\") for chunk in chunks]) or \"No relevant information found.\"\n",
    "    print(\"---CONTEXT RETRIEVED---\")\n",
    "    return state\n",
    "\n",
    "def synthesize_answer(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generates an answer based on retrieved context.\"\"\"\n",
    "    print(\"---NODE: SYNTHESIZE ANSWER---\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the user's question based *only* on the provided context.\"),\n",
    "        (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{query}\"),\n",
    "    ])\n",
    "    response_msg = (prompt | model).invoke({\"context\": state[\"relevant_context\"], \"query\": state[\"condensed_query\"]})\n",
    "    state[\"messages\"].append(response_msg)\n",
    "    return state\n",
    "\n",
    "def handle_conversation(state: AgentState) -> AgentState:\n",
    "    \"\"\"Handles conversational turns by explicitly passing the full chat history.\"\"\"\n",
    "    print(\"---NODE: HANDLE CONVERSATION---\")\n",
    "    query = state[\"condensed_query\"]\n",
    "    all_messages = state[\"messages\"]\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful and conversational AI assistant. Use the provided chat history to answer the user's question.\"),\n",
    "        (\"user\", \"Here is the chat history:\\n{history}\\n\\nBased on that history, please answer this question:\\n{question}\")\n",
    "    ])\n",
    "    history_string = \"\\n\".join(\n",
    "        f\"{msg.type.upper()}: {msg.content}\" for msg in all_messages\n",
    "    )\n",
    "    chain = prompt | model\n",
    "    response_msg = chain.invoke({\n",
    "        \"history\": history_string,\n",
    "        \"question\": query\n",
    "    })\n",
    "    state[\"messages\"].append(response_msg)\n",
    "    return state\n",
    "\n",
    "# --- Graph Definition (Your code is unchanged here) ---\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"condense_query\", condense_query)\n",
    "graph.add_node(\"retrieve_context\", retrieve_context)\n",
    "graph.add_node(\"synthesize_answer\", synthesize_answer)\n",
    "graph.add_node(\"handle_conversation\", handle_conversation)\n",
    "\n",
    "graph.set_entry_point(\"condense_query\")\n",
    "graph.add_conditional_edges(\"condense_query\", decide_route, {\n",
    "    \"use_retriever\": \"retrieve_context\",\n",
    "    \"general_conversation\": \"handle_conversation\",\n",
    "})\n",
    "graph.add_edge(\"retrieve_context\", \"synthesize_answer\")\n",
    "graph.add_edge(\"synthesize_answer\", END)\n",
    "graph.add_edge(\"handle_conversation\", END)\n",
    "\n",
    "agent = graph.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "# --- Main Chat Loop (Your code is unchanged here) ---\n",
    "if __name__ == \"__main__\":\n",
    "    conversation_id = str(uuid.uuid4())\n",
    "    print(f\"âœ… Agent started. Conversation ID: {conversation_id}\")\n",
    "    print(\"Type 'exit' or 'quit' to stop.\")\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            if not user_input or user_input.lower() in (\"exit\", \"quit\", \"q\"):\n",
    "                break\n",
    "            agent_input = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "            config = {\"configurable\": {\"thread_id\": conversation_id}}\n",
    "            final_state = agent.invoke(agent_input, config=config)\n",
    "            last_message = final_state[\"messages\"][-1]\n",
    "            print(f\"Bot: {last_message.content}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted. Exiting.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
